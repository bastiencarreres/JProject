{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1735ac",
   "metadata": {},
   "source": [
    "# Dowload lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c40eb1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign your Genius.com credentials and select your artist\n",
    "import lyricsgenius as genius\n",
    "token = \"CRG5jc-SgTRDpuJzU2neM_SajlResplv7DMi9jn1LmlNA-WiTvn2XseA_4rWb6y8\"\n",
    "artist_name = \"JuL\"\n",
    "geniusCreds = f\"{token}\"\n",
    "artist_name = f\"{artist_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de62c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for songs by JuL...\n",
      "\n",
      "Song 1: \"J’oublie tout\"\n",
      "Song 2: \"Tchikita\"\n",
      "Song 3: \"Tu la love\"\n",
      "Song 4: \"Dans ma paranoïa\"\n",
      "Song 5: \"My World\"\n",
      "Song 6: \"Wesh alors\"\n",
      "Song 7: \"Sors le cross volé\"\n",
      "Song 8: \"Lova\"\n",
      "Song 9: \"Comme d’hab\"\n",
      "Song 10: \"Dans mon dél\"\n",
      "Song 11: \"Tout seul\"\n",
      "Song 12: \"En Y\"\n",
      "Song 13: \"Toto et Ninetta\"\n",
      "Song 14: \"Au quartier\"\n",
      "Song 15: \"Amnésia\"\n",
      "Song 16: \"Mon bijou\"\n",
      "Song 17: \"Mon amour\"\n",
      "Song 18: \"Ma jolie\"\n",
      "Song 19: \"Le sang\"\n",
      "Song 20: \"Mama\"\n",
      "Song 21: \"Ils sont jaloux\"\n",
      "Song 22: \"Encore des paroles\"\n",
      "Song 23: \"Malade\"\n",
      "Song 24: \"On m’appelle l’ovni\"\n",
      "Song 25: \"Briganté\"\n",
      "Song 26: \"La classe\"\n",
      "Song 27: \"Señora\"\n",
      "Song 28: \"T’es pas le seul\"\n",
      "Song 29: \"Pourquoi tu me fais le gros\"\n",
      "Song 30: \"Coucou\"\n",
      "\n",
      "Reached user-specified song limit (30).\n",
      "Done. Found 30 songs.\n"
     ]
    }
   ],
   "source": [
    "Genius = genius.Genius(geniusCreds)\n",
    "artist = Genius.search_artist(artist_name, max_songs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb84259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics_JuL.json already exists. Overwrite?\n",
      "(y/n): y\n",
      "Wrote Lyrics_JuL.json.\n"
     ]
    }
   ],
   "source": [
    "artist.save_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a6ce520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open(f\"Lyrics_{artist_name}.json\")\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "Artist = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cddf12c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty dictionary to store your songs and related data\n",
    "artist_dict = {}\n",
    "def collectSongData(adic):\n",
    "    dps = list()\n",
    "    title = adic['title'] #song title\n",
    "    url = adic['url'] #spotify url\n",
    "    artist = adic['artist'] #artist name(s)\n",
    "    song_id = adic['id'] #spotify id\n",
    "    lyrics = adic['lyrics'] #song lyrics\n",
    "    upload_date = adic['description_annotation']['annotatable']['client_timestamps']['lyrics_updated_at'] #lyrics upload date\n",
    "    annotations = adic['annotation_count'] #total no. of annotations\n",
    "    descr = adic['description'] #song descriptions\n",
    "    \n",
    "    dps.append((title,url,artist,song_id,lyrics,upload_date,annotations,descr)) #append all to one tuple list\n",
    "    artist_dict[title] = dps #assign list to song dictionary entry named after song title\n",
    "    \n",
    "for song in Artist['songs']: \n",
    "    collectSongData(song) #check function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc03066b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['J’oublie tout', 'Tchikita', 'Tu la love', 'Dans ma paranoïa', 'My World', 'Wesh alors', 'Sors le cross volé', 'Lova', 'Comme d’hab', 'Dans mon dél', 'Tout seul', 'En Y', 'Toto et Ninetta', 'Au quartier', 'Amnésia', 'Mon bijou', 'Mon amour', 'Ma jolie', 'Le sang', 'Mama', 'Ils sont jaloux', 'Encore des paroles', 'Malade', 'On m’appelle l’ovni', 'Briganté', 'La classe', 'Señora', 'T’es pas le seul', 'Pourquoi tu me fais le gros', 'Coucou'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeaea523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input filename of song file, please add .csv\n",
      "julsong.csv\n",
      "30 songs have been uploaded\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def updateCSV_file():\n",
    "    upload_count = 0 #Set upload counter\n",
    "    location = f\"./\" #Pick file location\n",
    "    print(\"input filename of song file, please add .csv\")\n",
    "    filename = input() #give your file a name\n",
    "    file = location + filename\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as file: #open a new csv file\n",
    "        a = csv.writer(file, delimiter=',') #split by comma\n",
    "        #(title,url,artist,song_id,lyrics,year,upload_date,annotations,descr)\n",
    "        headers = [\"Title\",\"URL\",\"Artist\", \"Song ID\", \"Lyrics\", \"Upload Date\", \"Annotations\",\"Description\"] #create header row\n",
    "        a.writerow(headers) #add header row\n",
    "        for song in artist_dict:\n",
    "            a.writerow(artist_dict[song][0])\n",
    "            upload_count+=1\n",
    "            \n",
    "        print(str(upload_count) + \" songs have been uploaded\")\n",
    "updateCSV_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66bc60d",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922a13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from __future__ import print_function\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8240bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./julsong.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b70b129",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def split_text(x):\n",
    "    text = x['Lyrics']\n",
    "    sections = text.split('\\n\\n')\n",
    "    keys = {'Couplet 1': np.nan,'Couplet 2':np.nan,'Couplet 3':np.nan,'Couplet 4':np.nan, 'Refrain':np.nan}\n",
    "    \n",
    "    lyrics = str()\n",
    "\n",
    "    single_text = []\n",
    "\n",
    "    res = {}\n",
    "\n",
    "    for s in sections:\n",
    "\n",
    "        key = s[s.find('[') + 1:s.find(']')].strip()\n",
    "        \n",
    "        if ':' in key:\n",
    "            key = key[:key.find(':')]\n",
    "\n",
    "        if key in keys:\n",
    "            single_text += [x.lower().replace('(','').replace(')','').translate(translator) for x in s[s.find(']')+1:].split('\\n') if len(x) > 1]\n",
    "        \n",
    "\n",
    "        res['single_text'] =  ' \\n '.join(single_text)\n",
    "    return pd.Series(res)\n",
    "\n",
    "df = df.join(df.apply(split_text, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cbb1265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  9499\n",
      "Words with less than 7 appearances: 1436\n",
      "Words with more than 7 appearances: 232\n",
      "Valid sequences of size 5: 2546\n",
      "[['de', 'toi', '\\n', 'elle', 'taime'], ['et', 'la', 'nuit', 'elle', 'rêve'], ['sang', '\\n', 'jferai', 'tout', 'pour']]\n"
     ]
    }
   ],
   "source": [
    "# Compute word frequencies\n",
    "text_as_list = []\n",
    "\n",
    "frequencies = {}\n",
    "\n",
    "uncommon_words = set()\n",
    "\n",
    "MIN_FREQUENCY = 7\n",
    "\n",
    "MIN_SEQ = 5\n",
    "\n",
    "BATCH_SIZE =  32\n",
    "\n",
    "\n",
    "def extract_text(text):\n",
    "\n",
    "    global text_as_list\n",
    "\n",
    "    text_as_list += [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "\n",
    "df['single_text'].apply( extract_text )\n",
    "print('Total words: ', len(text_as_list))\n",
    "\n",
    "\n",
    "for w in text_as_list:\n",
    "    frequencies[w] = frequencies.get(w, 0) + 1\n",
    "\n",
    "uncommon_words = set([key for key in frequencies.keys() if frequencies[key] < MIN_FREQUENCY])\n",
    "words = sorted(set([key for key in frequencies.keys() if frequencies[key] >= MIN_FREQUENCY]))\n",
    "\n",
    "\n",
    "num_words = len(words)\n",
    "\n",
    "word_indices = dict((w, i) for i, w in enumerate(words))\n",
    "\n",
    "indices_word = dict((i, w) for i, w in enumerate(words))\n",
    "\n",
    "print('Words with less than {} appearances: {}'.format( MIN_FREQUENCY, len(uncommon_words)))\n",
    "\n",
    "print('Words with more than {} appearances: {}'.format( MIN_FREQUENCY, len(words)))\n",
    "\n",
    "\n",
    "valid_seqs = []\n",
    "\n",
    "end_seq_words = []\n",
    "\n",
    "for i in range(len(text_as_list) - MIN_SEQ ):\n",
    "\n",
    "    end_slice = i + MIN_SEQ + 1\n",
    "\n",
    "    if len( set(text_as_list[i:end_slice]).intersection(uncommon_words) ) == 0:\n",
    "\n",
    "        valid_seqs.append(text_as_list[i: i + MIN_SEQ])\n",
    "\n",
    "        end_seq_words.append(text_as_list[i + MIN_SEQ])\n",
    "\n",
    "\n",
    "print('Valid sequences of size {}: {}'.format(MIN_SEQ, len(valid_seqs)))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(valid_seqs, end_seq_words, test_size=0.02, random_state=42)\n",
    "\n",
    "print(X_train[2:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc17b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator for fit and evaluate\n",
    "\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, MIN_SEQ), dtype=np.int32)\n",
    "        y = np.zeros((batch_size), dtype=np.int32)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t] = word_indices[w]\n",
    "            y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n",
    "            index = index + 1\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(X_train+X_test))\n",
    "\n",
    "    seed = (X_train+X_test)[seed_index]\n",
    "\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "\n",
    "        for i in range(50):\n",
    "\n",
    "            x_pred = np.zeros((1, MIN_SEQ))\n",
    "\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t] = word_indices[word]\n",
    "\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "            examples_file.write(\" \"+next_word)\n",
    "\n",
    "        examples_file.write('\\n')\n",
    "\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "\n",
    "    examples_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966ce5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f50186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 17:37:12.776510: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-08-18 17:37:12.776542: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (marthes44): /proc/driver/nvidia/version does not exist\n",
      "2022-08-18 17:37:12.777462: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/80 [============================>.] - ETA: 0s - loss: 4.4334 - accuracy: 0.1578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/LSTM_LYRICS-epoch001-words232-sequence5-minfreq7-loss4.4135-acc0.1613-val_loss4.8434-val_acc0.1178/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints/LSTM_LYRICS-epoch001-words232-sequence5-minfreq7-loss4.4135-acc0.1613-val_loss4.8434-val_acc0.1178/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 31s 346ms/step - loss: 4.4135 - accuracy: 0.1613 - val_loss: 4.8434 - val_accuracy: 0.1178\n",
      "Epoch 2/20\n",
      "27/80 [=========>....................] - ETA: 1s - loss: 2.8010 - accuracy: 0.3704"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
    "            \"loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}\" % \\\n",
    "            (len(words), MIN_SEQ, MIN_FREQUENCY)\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, print_callback, early_stopping]\n",
    "\n",
    "\n",
    "examples_file = open('examples.txt', \"w\")\n",
    "\n",
    "model.fit(generator(X_train, y_train, BATCH_SIZE),\n",
    "                    steps_per_epoch=int(len(valid_seqs) / BATCH_SIZE) + 1,\n",
    "                    epochs=20,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=generator(X_test, y_train, BATCH_SIZE),\n",
    "                    validation_steps=int(len(y_train) / BATCH_SIZE) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ccad75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
