{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1735ac",
   "metadata": {},
   "source": [
    "# Dowload lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40eb1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign your Genius.com credentials and select your artist\n",
    "import lyricsgenius as genius\n",
    "token = \"CRG5jc-SgTRDpuJzU2neM_SajlResplv7DMi9jn1LmlNA-WiTvn2XseA_4rWb6y8\"\n",
    "artist_name = \"JuL\"\n",
    "geniusCreds = f\"{token}\"\n",
    "artist_name = f\"{artist_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de62c47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Genius = genius.Genius(geniusCreds)\n",
    "artist = Genius.search_artist(artist_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist.save_lyrics(ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6ce520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open(f\"Lyrics_{artist_name}_200.json\")\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "Artist = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a53c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "Artist['songs'][128]['lyrics'].split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf12c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty dictionary to store your songs and related data\n",
    "artist_dict = {}\n",
    "def collectSongData(adic):\n",
    "    dps = list()\n",
    "    title = adic['title'] #song title\n",
    "    url = adic['url'] #spotify url\n",
    "    artist = adic['artist'] #artist name(s)\n",
    "    song_id = adic['id'] #spotify id\n",
    "    lyrics = adic['lyrics'] #song lyrics\n",
    "    upload_date = adic['description_annotation']['annotatable']['client_timestamps']['lyrics_updated_at'] #lyrics upload date\n",
    "    annotations = adic['annotation_count'] #total no. of annotations\n",
    "    descr = adic['description'] #song descriptions\n",
    "    \n",
    "    dps.append((title,url,artist,song_id,lyrics,upload_date,annotations,descr)) #append all to one tuple list\n",
    "    artist_dict[title] = dps #assign list to song dictionary entry named after song title\n",
    "    \n",
    "for song in Artist['songs']: \n",
    "    collectSongData(song) #check function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaea523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def updateCSV_file():\n",
    "    upload_count = 0 #Set upload counter\n",
    "    location = f\"./\" #Pick file location\n",
    "    print(\"input filename of song file, please add .csv\")\n",
    "    filename = input() #give your file a name\n",
    "    file = location + filename\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as file: #open a new csv file\n",
    "        a = csv.writer(file, delimiter=',') #split by comma\n",
    "        #(title,url,artist,song_id,lyrics,year,upload_date,annotations,descr)\n",
    "        headers = [\"Title\",\"URL\",\"Artist\", \"Song ID\", \"Lyrics\", \"Upload Date\", \"Annotations\",\"Description\"] #create header row\n",
    "        a.writerow(headers) #add header row\n",
    "        for song in artist_dict:\n",
    "            a.writerow(artist_dict[song][0])\n",
    "            upload_count+=1\n",
    "            \n",
    "        print(str(upload_count) + \" songs have been uploaded\")\n",
    "updateCSV_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e13cdcb",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323258a0",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a066cfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Lyrics_JuL_200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5c6a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    text = text['Lyrics']\n",
    "    keys = re.findall(\"\\[(.*?)\\]\", text)[1:]\n",
    "    for k in keys:\n",
    "        text = text.replace(k, '@@')\n",
    "    sections = text.split('@@')    \n",
    "    \n",
    "    pattern = re.findall(r'\\d+Embed', sections[-1])\n",
    "    if len(pattern) == 0:\n",
    "        pattern = re.findall(r'Embed', sections[-1])\n",
    "    if len(pattern) > 0:\n",
    "        sections[-1] = sections[-1].replace(pattern[0],'')\n",
    "\n",
    "    lyrics = str()\n",
    "\n",
    "    single_text = []\n",
    "\n",
    "    res = {}\n",
    "    for s in sections:\n",
    "        single_text += [x.lower().replace('(','').replace(')','').translate(translator) for x in s[s.find(']')+1:].split('\\n') if len(x) > 1]\n",
    "\n",
    "    res['single_text'] =  ' \\n '.join(single_text)\n",
    "    return pd.Series(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d1004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df.apply(split_text, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66bc60d",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "922a13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from __future__ import print_function\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute word frequencies\n",
    "text_as_list = []\n",
    "\n",
    "frequencies = {}\n",
    "\n",
    "uncommon_words = set()\n",
    "\n",
    "MIN_FREQUENCY = 7\n",
    "\n",
    "MIN_SEQ = 5\n",
    "\n",
    "BATCH_SIZE =  32\n",
    "\n",
    "\n",
    "def extract_text(text):\n",
    "\n",
    "    global text_as_list\n",
    "\n",
    "    text_as_list += [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "\n",
    "df['single_text'].apply( extract_text )\n",
    "print('Total words: ', len(text_as_list))\n",
    "\n",
    "\n",
    "for w in text_as_list:\n",
    "    frequencies[w] = frequencies.get(w, 0) + 1\n",
    "\n",
    "uncommon_words = set([key for key in frequencies.keys() if frequencies[key] < MIN_FREQUENCY])\n",
    "words = sorted(set([key for key in frequencies.keys() if frequencies[key] >= MIN_FREQUENCY]))\n",
    "\n",
    "\n",
    "num_words = len(words)\n",
    "\n",
    "word_indices = dict((w, i) for i, w in enumerate(words))\n",
    "\n",
    "indices_word = dict((i, w) for i, w in enumerate(words))\n",
    "\n",
    "print('Words with less than {} appearances: {}'.format( MIN_FREQUENCY, len(uncommon_words)))\n",
    "\n",
    "print('Words with more than {} appearances: {}'.format( MIN_FREQUENCY, len(words)))\n",
    "\n",
    "\n",
    "valid_seqs = []\n",
    "\n",
    "end_seq_words = []\n",
    "\n",
    "for i in range(len(text_as_list) - MIN_SEQ):\n",
    "\n",
    "    end_slice = i + MIN_SEQ + 1\n",
    "\n",
    "    if len(set(text_as_list[i:end_slice]).intersection(uncommon_words) ) == 0:\n",
    "\n",
    "        valid_seqs.append(text_as_list[i: i + MIN_SEQ])\n",
    "\n",
    "        end_seq_words.append(text_as_list[i + MIN_SEQ])\n",
    "\n",
    "\n",
    "print('Valid sequences of size {}: {}'.format(MIN_SEQ, len(valid_seqs)))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(valid_seqs, end_seq_words, test_size=0.02, random_state=42)\n",
    "\n",
    "print(X_train[2:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.Series(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200, figsize=(25, 10))\n",
    "sdf = series.sort_values(ascending=False)[100:200]\n",
    "plt.scatter(sdf.index, sdf)\n",
    "plt.xticks(rotation = 45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator for fit and evaluate\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, MIN_SEQ), dtype=np.int32)\n",
    "        y = np.zeros((batch_size), dtype=np.int32)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t] = word_indices[w]\n",
    "            y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n",
    "            index = index + 1\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(X_train+X_test))\n",
    "\n",
    "    seed = (X_train+X_test)[seed_index]\n",
    "\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "\n",
    "        for i in range(50):\n",
    "\n",
    "            x_pred = np.zeros((1, MIN_SEQ))\n",
    "\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t] = word_indices[word]\n",
    "\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "            examples_file.write(\" \"+next_word)\n",
    "\n",
    "        examples_file.write('\\n')\n",
    "\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "\n",
    "    examples_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ce5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f50186",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
    "            \"loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}\" % \\\n",
    "            (len(words), MIN_SEQ, MIN_FREQUENCY)\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, print_callback, early_stopping]\n",
    "\n",
    "\n",
    "examples_file = open('examples.txt', \"w\")\n",
    "\n",
    "model.fit(generator(X_train, y_train, BATCH_SIZE),\n",
    "                    steps_per_epoch=int(len(valid_seqs) / BATCH_SIZE) + 1,\n",
    "                    epochs=200,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=generator(X_test, y_train, BATCH_SIZE),\n",
    "                    validation_steps=int(len(y_train) / BATCH_SIZE) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546951b6",
   "metadata": {},
   "source": [
    "# Seconde model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841bd184",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79ce2ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for lyrics in df.single_text:\n",
    "    text += '\\n' + lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c0d43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = text.lower().split(\" \\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65635b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da2a8216",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fd5bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input sequences using list of tokens\n",
    "input_sequences = []\n",
    "for line in corpus[1:]:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "\n",
    "    input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c44dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(keras.utils.pad_sequences(input_sequences,\n",
    "                           maxlen = max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a38cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80957c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5e79d5f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4b725eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 116, 50)           547200    \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 116, 300)         241200    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 116, 300)          0         \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 100)               160400    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5472)              552672    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10944)             59896512  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,397,984\n",
      "Trainable params: 61,397,984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(total_words, 50, \n",
    "                    input_length=max_sequence_len-1))\n",
    "\n",
    "# Add an LSTM Layer\n",
    "model.add(Bidirectional(LSTM(150, return_sequences=True)))  \n",
    "\n",
    "# A dropout layer for regularisation\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add another LSTM Layer\n",
    "model.add(LSTM(100))\n",
    "\n",
    "model.add(Dense(total_words / 2, activation='relu')) \n",
    "\n",
    "# In the last layer, the shape should be equal to the total number of words present in our corpus\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy']) \n",
    "\n",
    "#(# Pick a loss function and an optimizer)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "311/311 [==============================] - 166s 519ms/step - loss: 8.8973 - accuracy: 0.0173\n",
      "Epoch 2/100\n",
      "311/311 [==============================] - 172s 553ms/step - loss: 8.1372 - accuracy: 0.0178\n",
      "Epoch 3/100\n",
      "311/311 [==============================] - 189s 607ms/step - loss: 7.6840 - accuracy: 0.0183\n",
      "Epoch 4/100\n",
      "311/311 [==============================] - 184s 590ms/step - loss: 7.0908 - accuracy: 0.0226\n",
      "Epoch 5/100\n",
      "311/311 [==============================] - 192s 618ms/step - loss: 6.2435 - accuracy: 0.0417\n",
      "Epoch 6/100\n",
      "311/311 [==============================] - 191s 613ms/step - loss: 5.1545 - accuracy: 0.0783\n",
      "Epoch 7/100\n",
      "311/311 [==============================] - 205s 659ms/step - loss: 3.9049 - accuracy: 0.1811\n",
      "Epoch 8/100\n",
      "311/311 [==============================] - 193s 620ms/step - loss: 2.6735 - accuracy: 0.3607\n",
      "Epoch 9/100\n",
      "311/311 [==============================] - 178s 571ms/step - loss: 1.7403 - accuracy: 0.5579\n",
      "Epoch 10/100\n",
      "311/311 [==============================] - 188s 604ms/step - loss: 1.1146 - accuracy: 0.7112\n",
      "Epoch 11/100\n",
      "311/311 [==============================] - 183s 589ms/step - loss: 0.7147 - accuracy: 0.8052\n",
      "Epoch 12/100\n",
      "311/311 [==============================] - 204s 655ms/step - loss: 0.4635 - accuracy: 0.8709\n",
      "Epoch 13/100\n",
      "311/311 [==============================] - 216s 693ms/step - loss: 0.3162 - accuracy: 0.9160\n",
      "Epoch 14/100\n",
      "311/311 [==============================] - 220s 708ms/step - loss: 0.2545 - accuracy: 0.9311\n",
      "Epoch 15/100\n",
      "311/311 [==============================] - 195s 627ms/step - loss: 0.1967 - accuracy: 0.9481\n",
      "Epoch 16/100\n",
      "311/311 [==============================] - 191s 614ms/step - loss: 0.1673 - accuracy: 0.9558\n",
      "Epoch 17/100\n",
      "311/311 [==============================] - 204s 657ms/step - loss: 0.1551 - accuracy: 0.9579\n",
      "Epoch 18/100\n",
      "311/311 [==============================] - 193s 622ms/step - loss: 0.1575 - accuracy: 0.9586\n",
      "Epoch 19/100\n",
      "311/311 [==============================] - 189s 608ms/step - loss: 0.1583 - accuracy: 0.9548\n",
      "Epoch 20/100\n",
      "311/311 [==============================] - 191s 613ms/step - loss: 0.1684 - accuracy: 0.9533\n",
      "Epoch 21/100\n",
      "116/311 [==========>...................] - ETA: 1:56 - loss: 0.1161 - accuracy: 0.9706"
     ]
    }
   ],
   "source": [
    "history = model.fit(predictors, label, epochs= 100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./first_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69049d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lyrics(seed_text, next_words):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list],\n",
    "                     maxlen=max_sequence_len-1,padding='pre')        predicted = model.predict_classes(token_list, verbose=0)        output_word = \"\"        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
